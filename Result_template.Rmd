---
title: "Result_template"
author: "FÃ©lix Boudry"
date: "`r Sys.Date()`"
output: html_document
---

```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# `r paste(my_env$name)`

## Material and methods

### Subjects

The data used for this part of the work are characterized by the following 
features:

```{r}
analysis_data <- as.data.frame(analysis_data)
describe(x = analysis_data[c("age",
                             "height",
                             "weight",
                             # "train_years",
                             "train_volume",
                             "vo2_max")] , na.rm = T) %>%
  round() %>%
  select(c("mean", "sd", "median", "min", "max")) %>%
  my_table(
    caption = paste0(
      "<center><strong>Anthropological values (n = ",
      nrow(analysis_data) ,
      ")</center></strong>"
    )
  )
```

## Results

### Unsupervised learning

The unsupervised approach is based on algorithms that don't need labels to
create a classification. This mainly include clustering algorithms.

#### K-means clustering

```{r}
kclust_graph
kclust_coord
kclust_confusion
```

#### Hierarchical bottom-up clustering

```{r}
hclust_bu_graph
hclust_bu_confusion
```

#### Hierarchical top-down clustering

```{r}
hclust_td_graph
hclust_td_confusion
```

### Supervised learning

The supervised approach, compared to the unsupervised one, require labels to try 
a classification and the correct the classification process and criteria.

#### LightGBM

```{r}
lgbm_accuracy
lgbm_best_accuracy
lgbm_best_params
lgbm_confusion
t(as.data.frame(lgbm_model$params[c(
  "boosting",
  "lambda_l1",
  "lambda_l2",
  "num_leaves",
  "max_depth",
  "feature_fraction",
  "bagging_fraction",
  "bagging_freq",
  "learning_rate"
)])) %>% my_table()
lgbm_importance_plot %>%
  lgb.plot.importance()
lgbm_importance_plot %>%
  lgb.plot.interpretation()
lgbm_plot %>%
  walk(print)
```

#### XGBoost

```{r}
xgboost_accuracy
xgboost_best_accuracy
xgboost_best_params
xgboost_confusion
t(as.data.frame(xgboost_model$params[c(
  "boosting",
  "lambda_l1",
  "lambda_l2",
  "num_leaves",
  "max_depth",
  "feature_fraction",
  "bagging_fraction",
  "bagging_freq",
  "learning_rate"
)])) %>% my_table()
xgboost_importance_plot %>%
  xgb.plot.importance()
```

#### Keras

```{r}

```

#### PyTorch Tabular NODE

```{r}

```

## Models comparison

```{r}
result_list <-
  lst(
    results_lgbm = lst(
      lgbm_best_accuracy,
      lgbm_kappa,
      lgbm_f1,
      lgbm_auc_roc,
      lgbm_precision,
      lgbm_recall
    ),
    results_xgboost = lst(
      xgboost_best_accuracy,
      xgboost_kappa,
      xgboost_f1,
      xgboost_auc_roc,
      xgboost_precision,
      xgboost_recall
    )
  )

do.call(rbind, result_list) %>%
  `colnames<-`(c("Accuracy", "Kappa", "F1", "AUC-ROC", "Precision", "Recall")) %>%
  `rownames<-`(c("LGBM", "XGBoost")) %>%
  my_table()
```

